batch_size: 16                  # batch size
epochs: 200                     # total number of epochs
eval_every_n_epochs: 1          # validation frequency
# fine_tune_from: pretrained_gin  # sub directory of pre-trained model in ./ckpt
log_every_n_steps: 50           # print training log frequency
init_lr: 0.0001   # 0.00007             # initial learning rate for the prediction head
init_base_lr: 0.00008    #  0.001 # initial learning rate for the base GNN encoder
weight_decay: 0.01 #0.009               # weight decay of Adam
optimizer: adamw                 # optimizer
scheduler: cosine               # learning rate scheduler
warmup_ratio: 0.05
random_seed: 42
finetune_flag: True

pretrained_model: CLR                 # GNN backbone (i.e., gin/gcn)
drop_out: 0.1 #0.3

dataset:
  data_path: '/Users/bmw/Research/data/data/data.csv'
  num_workers: 0                # dataloader number of workers
  valid_size: 0.1               # ratio of validation data
  test_size: 0.1                # ratio of test data
  splitting: random           # data splitting (i.e., random/scaffold)