{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import SMILESVocab\n",
    "\n",
    "from utils import vae_loss\n",
    "from vae import GraphEncoder, SMILESDecoder, VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果: ['C', 'C', 'O']\n"
     ]
    }
   ],
   "source": [
    "test_smi = \"CCO\"\n",
    "tokens = SMILESVocab.tokenize_smiles(test_smi)\n",
    "print(\"分词结果:\", tokens)  # 应输出 ['C', 'C', 'O']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import build_vocab_large, preprocess_to_h5\n",
    "data_path = '/Users/bmw/Research/data/data/pubchem-10m-clean.txt'\n",
    "h5_path ='/Users/bmw/Research/data/data/pubchem.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Vocab: 9999918it [00:50, 199891.60it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_large(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:17:20] SMILES Parse Error: syntax error while parsing: [O][C][Branch1][C][C][C]\n",
      "[17:17:20] SMILES Parse Error: Failed parsing SMILES '[O][C][Branch1][C][C][C]' for input: '[O][C][Branch1][C][C][C]'\n"
     ]
    }
   ],
   "source": [
    "smi = '[O][C][Branch1][C][C][C]'\n",
    "mol = Chem.MolFromSmiles(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 442942it [14:28, 523.55it/s][12:50:03] Can't kekulize mol.  Unkekulized atoms: 14 15 16 17 18 27 28 29 30 31 32 33 34 35 36 37 38 39 40 49 50 51 52 53 54 55 56 57 66 67 68 69 70 71 72 73 74 83 84 85 86 87 88 89 90 91 100 101 102 103 104 105 106 107 108 109\n",
      "Preprocessing: 443048it [14:28, 521.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping due to error: wrong SMILES: CCCCCCCCC1=CC=C2C(=C3c4c5ccc(CCCCCCCC)ccc5c5c4c4c6c7c8c(c9ccc(CCCCCCCC)ccc9c8c8c9ccc(CCCCCCCC)ccc9c9c%10c%11ccc(CCCCCCCC)ccc%11c%11c%12c%13ccc(CCCCCCCC)ccc%13c5c%12c4c(c7c89)c%11%10)C2C36)C=C1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 6235190it [3:19:29, 536.10it/s][15:55:04] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61\n",
      "Preprocessing: 6235301it [3:19:29, 540.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping due to error: wrong SMILES: C1=CC=C2C(=C3c4c5cccccc5c5c4c4c6c7c8c(c9cccccc9c8c8c9cccccc9c9c%10c%11cccccc%11c%11c%12c%13cccccc%13c5c%12c4c(c7c89)c%11%10)C2C36)C=C1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 9999918it [5:16:45, 526.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total skipped molecules: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_to_h5(data_path, vocab, 'pubchem.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5 = h5py.File(h5_path, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys: ['data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File(h5_path, \"r\") as h5:\n",
    "    print(\"Available keys:\", list(h5.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys inside /data: ['edge_attr', 'edge_index', 'fgp', 'seq', 'x']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_path, \"r\") as h5:\n",
    "    data_group = h5[\"data\"]\n",
    "    print(\"Keys inside /data:\", list(data_group.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(h5_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m h5:\n\u001b[0;32m----> 2\u001b[0m     x_data \u001b[38;5;241m=\u001b[39m h5[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/x\u001b[39m\u001b[38;5;124m\"\u001b[39m][:]  \u001b[38;5;66;03m# Load entire dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of x:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/h5py/_hl/dataset.py:864\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    862\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(selection\u001b[38;5;241m.\u001b[39mmshape)\n\u001b[1;32m    863\u001b[0m fspace \u001b[38;5;241m=\u001b[39m selection\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m--> 864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mread(mspace, fspace, arr, mtype, dxpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dxpl)\n\u001b[1;32m    866\u001b[0m \u001b[38;5;66;03m# Patch up the output for NumPy\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_path, \"r\") as h5:\n",
    "    x_data = h5[\"data/x\"][:]  # Load entire dataset\n",
    "    print(\"Shape of x:\", x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_path = './models/ckpt/CLR_model_MolCLR.pth'\n",
    "CLRmodel = torch.load(state_dict_path, map_location=DEVICE, weights_only=False)\n",
    "graphmodel = CLRmodel.graphmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GraphEncoder(graphmodel)\n",
    "decoder = SMILESDecoder(vocab=vocab)\n",
    "vae = VAE(encoder=encoder, decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Dataset is random splitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data import MoleculeLoaderWrapper\n",
    "train_loader, val_loader, test_loader = MoleculeLoaderWrapper(batch_size=16, num_workers=0, valid_size=0.1,\n",
    "                                                              test_size=0.1, h5_path=h5_path, vocab = vocab,\n",
    "                                                               splitting='random' ).get_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/499996 [00:08<1175:38:41,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[434, 2], edge_index=[2, 930], edge_attr=[930, 2], batch=[434], ptr=[17])\n",
      "tensor(233.7912, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/499996 [00:08<500:34:59,  3.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[395, 2], edge_index=[2, 844], edge_attr=[844, 2], batch=[395], ptr=[17])\n",
      "tensor(239.2679, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/499996 [00:08<284:36:55,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[430, 2], edge_index=[2, 920], edge_attr=[920, 2], batch=[430], ptr=[17])\n",
      "tensor(224.5388, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[440, 2], edge_index=[2, 946], edge_attr=[946, 2], batch=[440], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/499996 [00:09<125:31:03,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(219.5494, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[376, 2], edge_index=[2, 800], edge_attr=[800, 2], batch=[376], ptr=[17])\n",
      "tensor(252.6078, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/499996 [00:09<91:00:46,  1.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[427, 2], edge_index=[2, 912], edge_attr=[912, 2], batch=[427], ptr=[17])\n",
      "tensor(238.9505, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[388, 2], edge_index=[2, 840], edge_attr=[840, 2], batch=[388], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/499996 [00:09<55:06:12,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(258.0616, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[421, 2], edge_index=[2, 904], edge_attr=[904, 2], batch=[421], ptr=[17])\n",
      "tensor(262.8052, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/499996 [00:09<45:48:32,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[418, 2], edge_index=[2, 888], edge_attr=[888, 2], batch=[418], ptr=[17])\n",
      "tensor(214.2092, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[373, 2], edge_index=[2, 790], edge_attr=[790, 2], batch=[373], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/499996 [00:10<35:34:09,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(254.6073, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[458, 2], edge_index=[2, 1000], edge_attr=[1000, 2], batch=[458], ptr=[17])\n",
      "tensor(241.6088, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/499996 [00:10<32:24:22,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[327, 2], edge_index=[2, 678], edge_attr=[678, 2], batch=[327], ptr=[17])\n",
      "tensor(270.4680, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[426, 2], edge_index=[2, 912], edge_attr=[912, 2], batch=[426], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/499996 [00:10<28:30:58,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(242.1198, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[389, 2], edge_index=[2, 830], edge_attr=[830, 2], batch=[389], ptr=[17])\n",
      "tensor(220.6996, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/499996 [00:11<27:11:27,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[358, 2], edge_index=[2, 768], edge_attr=[768, 2], batch=[358], ptr=[17])\n",
      "tensor(269.7538, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[368, 2], edge_index=[2, 800], edge_attr=[800, 2], batch=[368], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/499996 [00:11<26:02:05,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(269.3666, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[389, 2], edge_index=[2, 826], edge_attr=[826, 2], batch=[389], ptr=[17])\n",
      "tensor(234.7488, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/499996 [00:11<25:26:23,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[418, 2], edge_index=[2, 890], edge_attr=[890, 2], batch=[418], ptr=[17])\n",
      "tensor(258.9570, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[440, 2], edge_index=[2, 934], edge_attr=[934, 2], batch=[440], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/499996 [00:11<25:30:43,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(250.7289, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[334, 2], edge_index=[2, 718], edge_attr=[718, 2], batch=[334], ptr=[17])\n",
      "tensor(271.6482, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/499996 [00:12<25:03:33,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[495, 2], edge_index=[2, 1052], edge_attr=[1052, 2], batch=[495], ptr=[17])\n",
      "tensor(190.4420, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[377, 2], edge_index=[2, 812], edge_attr=[812, 2], batch=[377], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/499996 [00:12<24:39:37,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(253.2174, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[369, 2], edge_index=[2, 790], edge_attr=[790, 2], batch=[369], ptr=[17])\n",
      "tensor(271.0596, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/499996 [00:12<24:34:14,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[365, 2], edge_index=[2, 770], edge_attr=[770, 2], batch=[365], ptr=[17])\n",
      "tensor(262.1422, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[361, 2], edge_index=[2, 770], edge_attr=[770, 2], batch=[361], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/499996 [00:12<24:18:56,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(242.1682, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[370, 2], edge_index=[2, 790], edge_attr=[790, 2], batch=[370], ptr=[17])\n",
      "tensor(244.1611, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/499996 [00:13<25:09:05,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[409, 2], edge_index=[2, 894], edge_attr=[894, 2], batch=[409], ptr=[17])\n",
      "tensor(231.5224, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[327, 2], edge_index=[2, 684], edge_attr=[684, 2], batch=[327], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/499996 [00:13<24:50:52,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(258.8201, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[411, 2], edge_index=[2, 886], edge_attr=[886, 2], batch=[411], ptr=[17])\n",
      "tensor(211.0222, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/499996 [00:13<24:29:56,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[391, 2], edge_index=[2, 832], edge_attr=[832, 2], batch=[391], ptr=[17])\n",
      "tensor(255.9019, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[405, 2], edge_index=[2, 856], edge_attr=[856, 2], batch=[405], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/499996 [00:14<24:28:08,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(253.8355, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[472, 2], edge_index=[2, 1000], edge_attr=[1000, 2], batch=[472], ptr=[17])\n",
      "tensor(211.8339, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 33/499996 [00:14<24:28:03,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[371, 2], edge_index=[2, 782], edge_attr=[782, 2], batch=[371], ptr=[17])\n",
      "tensor(249.6889, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[391, 2], edge_index=[2, 826], edge_attr=[826, 2], batch=[391], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 35/499996 [00:14<24:40:32,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(255.9799, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[416, 2], edge_index=[2, 906], edge_attr=[906, 2], batch=[416], ptr=[17])\n",
      "tensor(221.7884, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 36/499996 [00:14<25:12:46,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[408, 2], edge_index=[2, 870], edge_attr=[870, 2], batch=[408], ptr=[17])\n",
      "tensor(225.8733, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[338, 2], edge_index=[2, 718], edge_attr=[718, 2], batch=[338], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38/499996 [00:15<24:57:15,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(284.4922, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[382, 2], edge_index=[2, 818], edge_attr=[818, 2], batch=[382], ptr=[17])\n",
      "tensor(225.4989, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 39/499996 [00:15<24:57:32,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[421, 2], edge_index=[2, 882], edge_attr=[882, 2], batch=[421], ptr=[17])\n",
      "tensor(253.1881, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[379, 2], edge_index=[2, 816], edge_attr=[816, 2], batch=[379], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/499996 [00:15<25:03:16,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(278.7012, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[421, 2], edge_index=[2, 898], edge_attr=[898, 2], batch=[421], ptr=[17])\n",
      "tensor(235.6891, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/499996 [00:15<25:00:59,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[422, 2], edge_index=[2, 922], edge_attr=[922, 2], batch=[422], ptr=[17])\n",
      "tensor(248.7969, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[463, 2], edge_index=[2, 994], edge_attr=[994, 2], batch=[463], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 44/499996 [00:16<24:49:48,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(232.8930, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[335, 2], edge_index=[2, 714], edge_attr=[714, 2], batch=[335], ptr=[17])\n",
      "tensor(275.6020, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/499996 [00:16<24:58:35,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[475, 2], edge_index=[2, 1028], edge_attr=[1028, 2], batch=[475], ptr=[17])\n",
      "tensor(244.3413, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[285, 2], edge_index=[2, 600], edge_attr=[600, 2], batch=[285], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 47/499996 [00:16<24:50:54,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(285.3435, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[349, 2], edge_index=[2, 730], edge_attr=[730, 2], batch=[349], ptr=[17])\n",
      "tensor(238.1800, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 48/499996 [00:16<24:41:50,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[430, 2], edge_index=[2, 920], edge_attr=[920, 2], batch=[430], ptr=[17])\n",
      "tensor(254.3987, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[383, 2], edge_index=[2, 816], edge_attr=[816, 2], batch=[383], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/499996 [00:17<24:19:20,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(225.6266, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[391, 2], edge_index=[2, 852], edge_attr=[852, 2], batch=[391], ptr=[17])\n",
      "tensor(251.4327, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/499996 [00:17<25:10:57,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[414, 2], edge_index=[2, 882], edge_attr=[882, 2], batch=[414], ptr=[17])\n",
      "tensor(221.0166, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[344, 2], edge_index=[2, 738], edge_attr=[738, 2], batch=[344], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 53/499996 [00:17<24:59:57,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(275.1084, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[403, 2], edge_index=[2, 858], edge_attr=[858, 2], batch=[403], ptr=[17])\n",
      "tensor(215.9088, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 54/499996 [00:17<24:53:19,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[421, 2], edge_index=[2, 916], edge_attr=[916, 2], batch=[421], ptr=[17])\n",
      "tensor(218.6539, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[407, 2], edge_index=[2, 860], edge_attr=[860, 2], batch=[407], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/499996 [00:18<24:44:55,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(211.3703, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[380, 2], edge_index=[2, 806], edge_attr=[806, 2], batch=[380], ptr=[17])\n",
      "tensor(225.7902, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 57/499996 [00:18<24:26:48,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[426, 2], edge_index=[2, 906], edge_attr=[906, 2], batch=[426], ptr=[17])\n",
      "tensor(238.6605, grad_fn=<AddBackward0>)\n",
      "DataBatch(x=[326, 2], edge_index=[2, 682], edge_attr=[682, 2], batch=[326], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 57/499996 [00:20<49:14:13,  2.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      4\u001b[0m seq \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 6\u001b[0m recon_logits, mu, logvar \u001b[38;5;241m=\u001b[39m vae(data, seq[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss(recon_logits, seq[:, \u001b[38;5;241m1\u001b[39m:], mu\u001b[38;5;241m=\u001b[39mmu, log_var\u001b[38;5;241m=\u001b[39mlogvar, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Research/Generate/vae.py:123\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x, tgt_seq)\u001b[0m\n\u001b[1;32m    121\u001b[0m mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m    122\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, log_var)\n\u001b[0;32m--> 123\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z, tgt_seq)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_hat, mu, log_var\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Research/Generate/vae.py:103\u001b[0m, in \u001b[0;36mSMILESDecoder.forward\u001b[0;34m(self, z, tgt_seq)\u001b[0m\n\u001b[1;32m    100\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(tgt_seq) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(positions)\n\u001b[1;32m    102\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(seq_len)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt_emb\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m), hidden_state, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)\n\u001b[1;32m    105\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    106\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:602\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    599\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 602\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[1;32m    603\u001b[0m         output,\n\u001b[1;32m    604\u001b[0m         memory,\n\u001b[1;32m    605\u001b[0m         tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    606\u001b[0m         memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    607\u001b[0m         tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    608\u001b[0m         memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    609\u001b[0m         tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    610\u001b[0m         memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1087\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m-> 1087\u001b[0m         x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n\u001b[1;32m   1088\u001b[0m     )\n\u001b[1;32m   1089\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\n\u001b[1;32m   1090\u001b[0m         x\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(\n\u001b[1;32m   1092\u001b[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m     )\n\u001b[1;32m   1095\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1116\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1102\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m   1108\u001b[0m         x,\n\u001b[1;32m   1109\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1115\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for data, seq, fgp in tqdm(train_loader):\n",
    "    print(data)\n",
    "    data = data.to(DEVICE)\n",
    "    seq = seq.to(DEVICE)\n",
    "\n",
    "    recon_logits, mu, logvar = vae(data, seq[:, :-1])\n",
    "    loss = vae_loss(recon_logits, seq[:, 1:], mu=mu, log_var=logvar, beta=0.5)\n",
    "    print(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 12\n",
      "Special Tokens:\n",
      "  <bos>: 8\n",
      "  <eos>: 9\n",
      "  <pad>: 10\n",
      "  <unk>: 11\n",
      "\n",
      "First 20 Chemical Tokens:\n",
      "  #: 0\n",
      "  1: 1\n",
      "  Br: 2\n",
      "  C: 3\n",
      "  Cl: 4\n",
      "  N: 5\n",
      "  O: 6\n",
      "  c: 7\n"
     ]
    }
   ],
   "source": [
    "vocab.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ClC(Br)N=O\n",
      "Decoded: ClC<unk>Br<unk>N<unk>O\n"
     ]
    }
   ],
   "source": [
    "test_smiles = \"ClC(Br)N=O\"\n",
    "encoded = vocab.encode(test_smiles, add_bos=True, add_eos=True)\n",
    "decoded = vocab.decode(encoded)\n",
    "print(f\"Original: {test_smiles}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 4, 3, 11, 2, 11, 5, 11, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import vae_loss, load_vocab_from_pickle, KLAnnealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab_from_pickle('/Users/bmw/Research/data/vae/pubchem_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 341\n",
      "Special Tokens:\n",
      "  <bos>: 337\n",
      "  <eos>: 338\n",
      "  <pad>: 339\n",
      "  <unk>: 340\n",
      "\n",
      "First 20 Chemical Tokens:\n",
      "  #: 0\n",
      "  %10: 1\n",
      "  %11: 2\n",
      "  %12: 3\n",
      "  %13: 4\n",
      "  %14: 5\n",
      "  %15: 6\n",
      "  %16: 7\n",
      "  %17: 8\n",
      "  %18: 9\n",
      "  %19: 10\n",
      "  %20: 11\n",
      "  %21: 12\n",
      "  %22: 13\n",
      "  %23: 14\n",
      "  %24: 15\n",
      "  %25: 16\n",
      "  %26: 17\n",
      "  %27: 18\n",
      "  %28: 19\n"
     ]
    }
   ],
   "source": [
    "vocab.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>TYPES</th>\n",
       "      <th>CO2</th>\n",
       "      <th>O2</th>\n",
       "      <th>N2</th>\n",
       "      <th>CO2/O2</th>\n",
       "      <th>CO2/N2</th>\n",
       "      <th>SELFIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nc1ccccc1</td>\n",
       "      <td>Conjugated Polymers</td>\n",
       "      <td>0.493917</td>\n",
       "      <td>0.119179</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>4.144323</td>\n",
       "      <td>38.06543</td>\n",
       "      <td>[N][C][=C][C][=C][C][=C][Ring1][=Branch1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CONc1ccccc1</td>\n",
       "      <td>Conjugated Polymers</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>0.201905</td>\n",
       "      <td>0.047007</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>22.55000</td>\n",
       "      <td>[C][O][N][C][=C][C][=C][C][=C][Ring1][=Branch1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCONc1ccccc1</td>\n",
       "      <td>Conjugated Polymers</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.106988</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>20.75000</td>\n",
       "      <td>[C][C][O][N][C][=C][C][=C][C][=C][Ring1][=Bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cn1cccc1</td>\n",
       "      <td>Conjugated Polymers</td>\n",
       "      <td>4.860000</td>\n",
       "      <td>1.650174</td>\n",
       "      <td>0.244970</td>\n",
       "      <td>2.945143</td>\n",
       "      <td>19.83915</td>\n",
       "      <td>[C][N][C][=C][C][=C][Ring1][Branch1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCCCCCCCCCCc1ccsc1</td>\n",
       "      <td>Conjugated Polymers</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>20.183070</td>\n",
       "      <td>9.402985</td>\n",
       "      <td>4.370000</td>\n",
       "      <td>9.38000</td>\n",
       "      <td>[C][C][C][C][C][C][C][C][C][C][C][C][C][C][=C]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>OCCCC</td>\n",
       "      <td>Thermally Rearranged Polymers</td>\n",
       "      <td>163.333333</td>\n",
       "      <td>20.993300</td>\n",
       "      <td>8.300095</td>\n",
       "      <td>7.780261</td>\n",
       "      <td>19.67849</td>\n",
       "      <td>[O][C][C][C][C]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>OCC</td>\n",
       "      <td>Thermally Rearranged Polymers</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>7.995365</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>8.630000</td>\n",
       "      <td>28.75000</td>\n",
       "      <td>[O][C][C]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>C[SiH](C1=CC=C([Si](C)(CC[Si](C)(CC)C)C)C=C1)C</td>\n",
       "      <td>Various Polymers</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>1.110803</td>\n",
       "      <td>0.257051</td>\n",
       "      <td>3.610000</td>\n",
       "      <td>15.60000</td>\n",
       "      <td>[C][SiH1][Branch2][Ring1][=C][C][=C][C][=C][Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>OCC</td>\n",
       "      <td>Various Polymers</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>2.690265</td>\n",
       "      <td>0.924933</td>\n",
       "      <td>19.886520</td>\n",
       "      <td>57.84205</td>\n",
       "      <td>[O][C][C]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>OC</td>\n",
       "      <td>Various Polymers</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>18.75000</td>\n",
       "      <td>[O][C]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>595 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             SMILES  \\\n",
       "0                                         Nc1ccccc1   \n",
       "1                                       CONc1ccccc1   \n",
       "2                                      CCONc1ccccc1   \n",
       "3                                          Cn1cccc1   \n",
       "4                               CCCCCCCCCCCCc1ccsc1   \n",
       "..                                              ...   \n",
       "590                                           OCCCC   \n",
       "591                                             OCC   \n",
       "592  C[SiH](C1=CC=C([Si](C)(CC[Si](C)(CC)C)C)C=C1)C   \n",
       "593                                             OCC   \n",
       "594                                              OC   \n",
       "\n",
       "                             TYPES         CO2         O2        N2  \\\n",
       "0              Conjugated Polymers    0.493917   0.119179  0.012975   \n",
       "1              Conjugated Polymers    1.060000   0.201905  0.047007   \n",
       "2              Conjugated Polymers    2.220000   0.620112  0.106988   \n",
       "3              Conjugated Polymers    4.860000   1.650174  0.244970   \n",
       "4              Conjugated Polymers   88.200000  20.183070  9.402985   \n",
       "..                             ...         ...        ...       ...   \n",
       "590  Thermally Rearranged Polymers  163.333333  20.993300  8.300095   \n",
       "591  Thermally Rearranged Polymers   69.000000   7.995365  2.400000   \n",
       "592               Various Polymers    4.010000   1.110803  0.257051   \n",
       "593               Various Polymers   53.500000   2.690265  0.924933   \n",
       "594               Various Polymers    0.450000   0.090000  0.024000   \n",
       "\n",
       "        CO2/O2    CO2/N2                                            SELFIES  \n",
       "0     4.144323  38.06543          [N][C][=C][C][=C][C][=C][Ring1][=Branch1]  \n",
       "1     5.250000  22.55000    [C][O][N][C][=C][C][=C][C][=C][Ring1][=Branch1]  \n",
       "2     3.580000  20.75000  [C][C][O][N][C][=C][C][=C][C][=C][Ring1][=Bran...  \n",
       "3     2.945143  19.83915               [C][N][C][=C][C][=C][Ring1][Branch1]  \n",
       "4     4.370000   9.38000  [C][C][C][C][C][C][C][C][C][C][C][C][C][C][=C]...  \n",
       "..         ...       ...                                                ...  \n",
       "590   7.780261  19.67849                                    [O][C][C][C][C]  \n",
       "591   8.630000  28.75000                                          [O][C][C]  \n",
       "592   3.610000  15.60000  [C][SiH1][Branch2][Ring1][=C][C][=C][C][=C][Br...  \n",
       "593  19.886520  57.84205                                          [O][C][C]  \n",
       "594   5.000000  18.75000                                             [O][C]  \n",
       "\n",
       "[595 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/bmw/Research/data/data/data.csv')\n",
    "df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for smi in df['SMILES']:\n",
    "    encoded = vocab.encode(test_smi, add_bos=True, add_eos=True)\n",
    "    decoded = vocab.decode(encoded)\n",
    "    # print(f\"Original: {test_smi}\")\n",
    "    # print(f\"Decoded: {decoded}\")\n",
    "    results.append(decoded == test_smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
